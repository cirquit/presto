# renamed file from "data_utility" in the CREAM repository, from the paper "CREAM, a component level coffeemaker electrical activity measurement dataset" by author Daniel Jorde

# Type annotation imports
from typing import Union
from typing import Tuple
# Other imports
import os
import glob
import h5py
from datetime import datetime
from datetime import timedelta
from datetime import timezone
import numpy as np
import pandas as pd
from scipy import interpolate
#-------------------------------------------------------------------------------------------------------#
# CREAM Data Utility class. Please refer to the docstring for details!
#-------------------------------------------------------------------------------------------------------#

class CREAM_Day():
    """
    A class representing one particular day of the CREAM dataset.
    
    The CREAM dataset has the following file structure:
    |-CREAM
    |------- 2018-08-23 
    |        |--------- *.hdf5
    |        |--------- *.hdf5
    |        |--------- *.hdf5
    |        |--------- ......
    |
    |------- 2018-08-24
    .......
    
    This class corresponds to one of the subfolders, i.e. of the folders representing a particular day, such as, for
    example, the first folder "2018-08-23". You have to create one CREAM_Day object per day folder in order to use the 
    data of the full dataset. 

     During initialization, the following attributes are set.

    files_metadata_df (pandas.DataFrame): columns: Start_timestamp, End_timestamp, Filename to store start
                                        end times of each file in this day
    files (list): full path to every file in this day
    minimum_request_timestamp (datetime.datetime): First timestamp of the day
    maximum_request_timestamp (datetime.datetime): Last timestamp of the day

    file_cache (dict): file cache for buffering already loaded files
    day_date (datetime.datetime): day and date of the current object


    This class also provides convenience functions to load the files of the CREAM dataset.
    To load an arbitrary CREAM file, use the load_file method.
    To load an arbitrary data window, based on the start_timestamp of the window to load, use the load_time_frame method.
    To load the maintenance or product events as a pandas.DataFrame, use the load_machine_events method.
    Via a parameter, one can also load the raw files that were generated by the coffee maker (they can be found in the
    raw_coffee_maker_logs subfolder of the CREAM dataset).
    To load the component events as a pandas.DataFrame, use the load_component_events_method.
    To load information whether a specific day is a working day (German working day in the dataset), use the get_weekday_
    information method.


    Other self-explaining convenience functions are:
        - get_datetime_from_filepath
        - get_index_from_timestamp
        - get_timestamp_from_index

    Functions starting with an "_" underscore are private functions and are not intended for user usage.


    """

    def __init__(self, cream_day_location: str, use_buffer : bool =False, buffer_size_files : int =5):
        """
        Initialize the CREAM_Day object
        Parameters
        ----------
        cream_day_location (str): location of the root folder of the respective day in the CREAM dataset. Specify
                                a path to the respective day, not to the root of the overall CREAM datset!
        use_buffer (boolean): default=False. In case it is set to True, files loaded via the load_file, or load_time_frame
                            method are stored in the cache of the CREAM_Day object. This speeds up streaming the dataset.
                            In case no buffer_size_file is provide, a default buffer_size_files of 5 is used.
                            Hence, the recent 5 files are stored in the cache. Old files are automatically removed from
                            the cache in case the buffer_size_files limit is exceeded.
        buffer_size_files (int): Size of the file cache of the CREAM_Day object. Functionality of the cache is documented
                            in the use_buffer parameter description right above.
        """
        self.dataset_location = cream_day_location
        self.use_buffer = use_buffer
        self.buffer_size_files = buffer_size_files

        if self.buffer_size_files == 5 and use_buffer is True:
            raise Warning("Buffer size was specified with size 5 (default value): a minimum buffer size of 5 files was set therefore")

        # Initiate the file buffer dictionary
        self.file_cache = {}

        #print(f"Loc: {self.dataset_location}")
        p = os.path.join(self.dataset_location, "*.hdf5")
        #print(f"Loc-joined: {p}")
        # Get all the files of the respective day
        self.files = glob.glob(os.path.join(self.dataset_location, "*.hdf5"))
        self.files.sort()
        #print(f"Files: {self.files}")
        # We use the first file and the timestamps in the filenames in the dataset (of this day) to get the metadata information
        # Get the timezone information from the filename timestamp

        # Load Metadata from the first file of the respective device --> same for all of the device --> STATIC METADATA
        with h5py.File(self.files[0], 'r', driver='core') as f:

            self.sampling_rate = int(f.attrs['frequency'])  # get the sampling rate
            self.samples_per_file = len(f["voltage"])  # get the length of the signal

            # get the start timestamp‚
            start_timestamp = datetime(
                year=int(f.attrs['year']),
                month=int(f.attrs['month']),
                day=int(f.attrs['day']),
                hour=int(f.attrs['hours']),
                minute=int(f.attrs['minutes']),
                second=int(f.attrs['seconds']),
                microsecond=int(f.attrs['microseconds']),
                tzinfo=timezone(timedelta(hours=int(f.attrs['timezone'][1:4]), minutes=int(f.attrs['timezone'][4:]))))

            self.file_duration_sec = 60 * 60 # each file, one hour --> seconds per file
            self.number_of_files = len(self.files)

        # Some file metadata for every file
        file_start_times = [self.get_datetime_from_filepath(f) for f in self.files]

        file_end_times = [timedelta(seconds=self.file_duration_sec) + ts for ts in file_start_times]
        self.files_metadata_df = pd.DataFrame({"Start_timestamp": file_start_times,
                                               "Filename": self.files,
                                               "End_timestamp": file_end_times})


        self.dataset_name = "CREAM"


        # Compute the minimum and maximum time for this day, and the respective differences to the day before
        self.minimum_request_timestamp = self.files_metadata_df.iloc[0].Start_timestamp
        self.maximum_request_timestamp = self.files_metadata_df.iloc[-1].Start_timestamp + timedelta(seconds=self.file_duration_sec)

        # Find the day of the dataset
        folder_path = os.path.basename(os.path.normpath(self.dataset_location))  # name of the folder
        date = folder_path.split("-")
        self.day_date = datetime(year=int(date[0]), month=int(date[1]), day=int(date[2]))

        # Initialize weekday information
        self.weekday_information_df = None

    def load_machine_events(self, file_path: str = None, filter_day : bool = False, raw_file=True) -> pd.DataFrame:
        """
        Load the maintenance event file. The events are sorted by the time they occur.

        Parameters
        ----------
        file_path (str): path to the component events file (.csv) file
        filter_day (boolean): default=False. If set to True, the DataFrame is filtered for the events belonging
                            to the CREAM_Day object
        raw_file (boolean): default=True. If set to True, the user has to provide the path to the raw events file that
                            were generated by the coffee maker. They can be found in the raw_coffee_maker_logs subfolder
                            of the dataset.


        Returns
        -------
        data (pd.DataFrame):
                    if raw_file=True: pd.DataFrame with columns "Timestamp", "Activity" (maintenance file) or
                    "Timestamp", "Product" (product file)
                    If raw_file=False: pd.DataFrame with columns
                    'Start_Timestamp', 'Automatic_Timestamp', 'Event_Type', 'End_Timestamp', 'Event_Duration_Seconds', 'Date',
                    Sorted descending by 'Start_Timestamp'.
        """



        if file_path is None:
            raise ValueError("Specify a file_path, containing the events file.")

        if raw_file is True and "raw" not in file_path:
            raise ValueError("In case you intend to load a raw_file, you also need to pass a path to a raw file to the "
                             "function!")

        data = pd.read_csv(file_path)

        # The timezone of the timestamps need to be from the same type
        # We use the first file of the day_object to get
        timezone = self.get_datetime_from_filepath(self.files[0]).tzinfo

        if raw_file is True:  # In case the raw product file is used

            data.Timestamp = pd.to_datetime(data.Timestamp)

            data = self._convert_timezone(data, "Timestamp", target_timezone=timezone)

            data.sort_values("Timestamp", inplace=True)

            data["Date"] = data.Timestamp.apply(lambda x: x.date())

        else:  # the manually adjusted and pre-processed product file is used

            for column in data.columns:

                # Convert all timestamp columns
                if "Timestamp" in column:
                    data[column] = pd.to_datetime(data[column])
                    data = self._convert_timezone(data, column, target_timezone=timezone)

            data["Date"] = data.End_Timestamp.apply(lambda x: x.date())

            data.sort_values("Start_Timestamp", inplace=True)


        if filter_day is True:  # only return the event of the corresponding CREAM day
            data = data[data["Date"] == self.day_date.date()]

        return data

    def load_component_events(self, file_path: str = None, filter_day : bool = False) -> pd.DataFrame:
        """
        Load the labeled electrical events, i.e. the components events, file. The events are sorted by the time they occur.

        Parameters
        ----------
        file_path (str): path to the component events file (.csv) file
        filter_day (boolean): default=False, if set to True, the DataFrame is filtered for the events belonging
                            to the CREAM_Day object


        Returns
        -------
        data (pd.DataFrame): pd.DataFrame with columns:
            'Start_Timestamp', 'Automatic_Timestamp', 'Event_Type', 'End_Timestamp', 'Event_Duration_Seconds', 'Date',
            Sorted descending by 'Start_Timestamp'.
        """


        if file_path is None:
            raise ValueError("Specify a file_path, containing the events file.")

        data = pd.read_csv(file_path)
        # The timezone of the timestamps need to be from the same type
        # We use the first file of the day_object to get
        timezone = self.get_datetime_from_filepath(self.files[0]).tzinfo

        for column in data.columns:

            # Convert all timestamp columns
            if "Timestamp" in column:
                data[column] = pd.to_datetime(data[column])
                data = self._convert_timezone(data, column, target_timezone=timezone)

        data["Date"] = data.Timestamp.apply(lambda x: x.date())

        data.sort_values("Timestamp", inplace=True)

        if filter_day is True:  # only return the event of the corresponding CREAM day
            data = data[data["Date"] == self.day_date.date()]

        return data

    def load_file(self, file_path: str, return_noise: bool = False) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load a file of the CREAM dataset
        If return_noise is specified, the noise channel is also returned. The current is 2-dimensional then.
        The signals get pre-processed before they are returned by this function:
        1. y-direction calibration: we center the signal around zero
        2. calibration_factor: we calibrate the signal by the measurement device specific calibration_factor.
        This calibration_factor is included in the metadata of the files.

        Parameters
        ----------
        file_path (string): path to the file to be loaded
        return_noise (boolean): default=False. If set to True, the current of the noise socket is also returned.

        Returns
        -------
        voltage (ndarray): voltage signal with shape=(1, file_length,). In case of an empty file None is returned.
        current (ndarray): current signal either with shape (1, file_length) or (2, file_length)
                            In case of an empty file None is returned


        """

        voltage = None
        current = None

        # Check if the file is already in the file cache
        if self.use_buffer is True and file_path in self.file_cache:

            voltage = self.file_cache[file_path]["voltage"]
            current = self.file_cache[file_path]["current"]

            return voltage, current

        else:


            # Check if the file is empty (zero bytes): if so return and empty current and voltage array
            if os.stat(file_path).st_size > 0:  # if not empty

                with h5py.File(file_path, 'r', driver='core') as f:

                    voltage_offset, current_offset = self._adjust_amplitude_offset(f)  # y value offset adjustment
                    for name in list(f):
                        signal = f[name][:] * 1.0

                        if name == 'voltage' and voltage_offset is not None:  # the voltage signal

                            voltage = signal - voltage_offset
                            calibration_factor = f[name].attrs['calibration_factor']
                            voltage = np.multiply(voltage, calibration_factor)

                        elif "current1" in name and current_offset is not None:  # the current signal of the coffee maker

                            current = signal - current_offset
                            calibration_factor = f[name].attrs['calibration_factor']
                            current = np.multiply(current, calibration_factor)

                        elif return_noise == True and "current6" in name and current_offset is not None:  # the current signal of the noise channel

                            current_noise = signal - current_offset
                            calibration_factor = f[name].attrs['calibration_factor']
                            current_noise = np.multiply(current_noise, calibration_factor)

                if return_noise is True:
                    current = np.array([current, current_noise])
                    voltage = np.array(voltage)
                else:
                    current = np.array(current)
                    voltage = np.array(voltage)

                # Before returning, check if we store the file in the cache and if we need to delete one instead from the cache
                if self.use_buffer is True:
                    if len(self.file_cache) < self.buffer_size_files:
                        self.file_cache[file_path] = {"voltage" : np.array(voltage), "current": np.array(current)}

                    else:
                        sorted_filenames = list(self.file_cache.keys())
                        sorted_filenames.sort()
                        del self.file_cache[sorted_filenames[0]] #delete the oldest file

                return np.array(voltage), np.array(current)

            else:  # if empty
                return None, None

    def load_file_metadata(self, file_path: str, attribute_list: list = []) -> dict:
        """
        Load the file metadata for a specifc files.
        The metadata is stored in the HDF5 attributes, details are documented in the data descriptor.
        The following attributes are available:
        ["name", "first_trigger_id", "last_trigger_id", "sequence", "frequency", "year", "month", "day",
        "hours", "minutes", "seconds", "microseconds", "timezone", "calibration_factor", "removed_offset"]

        Parameters
        ----------
        file_path (str): path to the file to be loaded. Needs to be the full-path, as provide by the "files"
                        attribute of the CREAM_Day object.
        attribute_list (list): default=[], specify specifc attribute names to be loaded. If no
                                dedicated attributes are specified, all attributes are returned

        Returns
        -------
        attributes_dict (dict): dictionary with all HDF5 attributes of a specifc file.

        """

        if file_path is None:
            raise ValueError("Specify a file path!")

        all_attributes = ["name", "first_trigger_id", "last_trigger_id", "sequence", "frequency", "year", "month", "day",
        "hours", "minutes", "seconds", "microseconds", "timezone", "calibration_factor", "removed_offset"]

        if len(attribute_list) == 0: #use all attributes if non is specified
            attribute_list = all_attributes
        else:
            # Check if user specified attributes exist in the metadata
            for attr in attribute_list:
                if attr not in all_attributes:
                    raise ValueError("The atttribute %s is not available!")

        attributes_dict = {}
        with h5py.File(file_path, 'r', driver='core') as f:

            for attr in attribute_list:

                if attr in ["calibration_factor", "removed_offset"]: #not in the attribute root of the hdf5 file
                    attributes_dict[attr] = f["voltage"].attrs[attr]
                else: #attributes in the root of the hdf5 file
                    attributes_dict[attr] = f.attrs[attr]

        return attributes_dict

    def load_time_frame(self, start_datetime: datetime, duration : float, return_noise: bool = False) -> Tuple[np.ndarray, np.ndarray]:

        """
        Loads an arbitrary time-frame of the CREAM dataset. Can be also used for streaming the data fast: in case
        the caching parameter is enabled in the CREAM_Day object. Otherwise, the files will be reloaded every time
        this method is called, thus, slowing down the data retrieval.
        Parameters
        ----------
        start_datetime (datetime.datetime): start timestamp of the window to load
        duration (float): duration of the window to load (window size) in seconds. ATTENTION: if not provided in seconds,
                        wrong results are returned!
        return_noise (boolean): default: False. If set to True, also returns the signal of the noise channel recorded in
                                CREAM dataset (from socket 6)

        Returns
        -------
        voltage (numpy.ndarray): voltage signal of the window
        current (numpy.ndarray): curent signal of the window. One dimensional if return_noise=False, two dimensional if
                                if return_noise=True. The first element is the coffee-maker signal, the second element
                                the noise signal.

        """

        # Perform initial checks
        if start_datetime < self.minimum_request_timestamp:
            raise ValueError(
                "The requested Time window is smaller then the minimum_request_timestamp of the day object")

        end_datetime = start_datetime + timedelta(seconds=duration)

        if end_datetime > self.maximum_request_timestamp:
            raise ValueError("The requested Time window is bigger then the maximum_request_timestamp of the day object")


        # determine all the files that are relevant for the requested time window

        # The index of the first relevant_file: i.e. the last file that is smaller then the start_datetime
        first_file_idx = self.files_metadata_df[self.files_metadata_df.Start_timestamp <= start_datetime].index[-1]

        # The last relevant_file: i.e. the first file that has and End_timestamp that is bigger then the one we need
        last_file_idx = self.files_metadata_df[self.files_metadata_df.End_timestamp >= end_datetime].index[0]


        # Get all the files in between the first and the last file needed
        relevant_files_df = self.files_metadata_df.loc[first_file_idx:last_file_idx]

        if len(relevant_files_df) == 0:
            raise ValueError("The timeframe requested does not lie within the current day!")

        relevant_voltage = []
        relevant_current = []
        relevant_current_noise = []

        for i, row in relevant_files_df.iterrows():

            voltage, current = self.load_file(row.Filename, return_noise=return_noise)

            relevant_voltage.append(voltage)
            relevant_current.append(current)

            if return_noise is True:
                relevant_current.append(current[0])
                relevant_current_noise.append(current[1])

        # now stack together the relevant signals
        relevant_voltage = np.concatenate(relevant_voltage, axis=-1)
        relevant_current = np.concatenate(relevant_current, axis=-1)

        if return_noise is True and len(relevant_current_noise) > 0:
            relevant_current_noise = np.concatenate(relevant_current_noise, axis=-1)

        # Compute the start_index

        # 1.1 Compute the offset in the first file
        start_index = int(self.get_index_from_timestamp(relevant_files_df.iloc[0].Start_timestamp, start_datetime))
        end_index = int(self.get_index_from_timestamp(relevant_files_df.iloc[0].Start_timestamp, end_datetime))

        # Get the voltage and current window
        voltage = relevant_voltage[start_index:end_index] #there is only one voltage channel

        if return_noise is True and len(relevant_current_noise) > 0:
            current = [relevant_current[start_index:end_index], relevant_current_noise[start_index:end_index]]
        else:
            current = relevant_current[start_index:end_index]

        voltage = np.array(voltage)
        current = np.array(current)

        return voltage, current

    def compute_average_sampling_rate(self) -> float:
        """
        Estimate the average sampling rate per day.
        Load the metadata of every file of the current day.
        Per file (one hour files), we compute the actual sampling rate.
        We then average this number over all files of this day, resulting in the average sampling rate.

        Calculate the difference between the first and last sample of a day based on
        the timestamps of the files.
        Sets the average_sampling_rate attribute of the CREAM_Day object.
        One can compare the average_sampling_rate to the nominal one of 6400.

        Parameters
        ----------

        Returns
        -------
        average_sampling_rate (float): average sampling rate per day (computed over the files)
        """

        FILE_LENGTH_SEC = 60 * 60 #one hour files
        actual_sampling_rates = []
        for file in self.files:
            voltage, current = self.load_file(file_path=file)
            samples_per_file = len(voltage)
            actual_sampling_rate = samples_per_file / FILE_LENGTH_SEC
            actual_sampling_rates.append(actual_sampling_rate)
        self.average_sampling_rate = np.mean(actual_sampling_rates)

        return self.average_sampling_rate

    def get_datetime_from_filepath(self, filepath: str) -> datetime:
        """
        Extracts the datetime from a filename of a CREAM file.
        Parameters
        ----------
        filepath (str): path to a CREAM file

        Returns
        -------
        start_timestamp (datetime): start timestamp of the file, extracted from the filename
        """

        filename = os.path.basename(filepath)  # get the filename
        string_timestamp = "-".join(filename.split("-")[2:-1])

        datetime_object = datetime.strptime(string_timestamp, '%Y-%m-%dT%H-%M-%S.%fT%z')  # string parse time

        return datetime_object

    def get_index_from_timestamp(self, start_timestamp: datetime, event_timestamp: datetime) -> int:
        """
        Returns the index of the event, represented by the event_timestamp, relativ to the start_timestamp (i.e. start timestamp of the file of interest e.g.)
        Parameters
        ----------
        start_timestamp (datetime.datetime): start timestamp of the window the event is located at
        event_timestamp (datetime.datetime): timestamp of the event of interest

        Returns
        -------
        event_index (int): The resulting event index

        """
        sec_since_start = event_timestamp - start_timestamp
        event_index = sec_since_start.total_seconds() * (self.sampling_rate)  # and # multiply by samples per second

        return int(event_index)

    def get_timestamp_from_index(self, start_timestamp: datetime, event_index: int) -> datetime:
        """
        Returns the timestamp for an event index. The event index has to be relative to a start_timestamp of a window.
        Parameters
        ----------
        start_timestamp (datetime.datetime): start timestamp of the window.
        event_index (int): Index of the event of interest, has to be relative to the start_timestamp provided.

        Returns
        -------
        event_timestamp (datetime.datetime): The resulting timestamp

        """

        seconds_per_sample = 1 / self.sampling_rate # 1 second / samples = seconds per sample
        time_since_start = event_index * seconds_per_sample
        event_ts = start_timestamp + timedelta(seconds=time_since_start)

        return event_ts

    def _adjust_amplitude_offset(self, file: h5py.File) -> Tuple[int, int]:
        """
        Resembles the pre-processing functionality in the BLOND repository (one_second_data_summary_functions.py) by
        Thomas Kriechbaumer.


        Computes the mean per period to get an estimate for the offset in each period.
        This is done for the voltage signal.
        The period length is computed using the nominal sampling rate. Tthis can deviate from the
        actual period length. Therefore, we zero pad the voltage signal to get full periods again before computing
        the mean.
        Then we use the estimate per period, to linearly interpolate the mean values per period, to get an offset value
        per sample point in the signal. We then use the offset of the voltage to compute the offset of the current by multiplying
        it by the crest-coefficient of 1/sqrt(2), i.e., approx. 0.7 .
        Parameters
        ----------
        file (h5py.File): a h5py CREAM file.

        Returns
        -------
        voltage_offset (int): the voltage offset to adjust for
        current_offset (int): the current offset to adjust for

        """

        length = len(file['voltage'])

        # Compute the average period_length, using the nominal sampling rate
        period_length = round(self.sampling_rate / 50)

        # Get the missing samples, opposed to the optimal number of periods in the signal
        remainder = divmod(length, period_length)[1]


        voltage = np.pad(file['voltage'][:], (0, period_length - remainder), 'constant',
                             constant_values=0)  # zero padding

        voltage = voltage.reshape(-1, period_length)  # the single periods, period wise reshape
        mean_values_per_period = voltage.mean(axis=1)  # compute the mean per period

        # Create x values for the interpolation
        x_per_period = np.linspace(1, length, len(mean_values_per_period), dtype=np.int)  # number of periods
        x_original = np.linspace(1, length, length, dtype=np.int)

        # build a linear interpolation, that interpolates for each period witch offset it should have
        # for each of the datapoints, interpolate the offset
        voltage_offset = interpolate.interp1d(x_per_period, mean_values_per_period)(x_original)
        current_offset = voltage_offset * 1 / np.sqrt(2)  # roughly * 0.7

        return voltage_offset, current_offset

    def _convert_timezone(self, dataframe: pd.DataFrame, column_name : str, target_timezone:str) -> pd.DataFrame:
        """
        Converts timezone in column_name column in dataframe to target_timezone

        Parameters
        ----------
        dataframe (pandas.DataFrame): DataFrame object, containing some time columns
        column_name (str): Name of the column of interest, i.e. the name of a time column
        target_timezone (str): datetime.datetime.tzinfo timezone information as a string. This is the target timezone.

        Returns
        -------
        dataframe (pandas.DataFrame): DataFrame object, with the column_name column converted to the target_timezone

        """

        ts_array = []

        for i, row in dataframe.iterrows():
            ts = row[column_name].tz_convert(target_timezone)
            ts_array.append(ts)

        dataframe[column_name] = ts_array

        return dataframe

    def get_weekday_information(self,  date : Union[list, np.ndarray], file_path : str = None) -> pd.DataFrame:

        """
        For a certain date, get the day related information from the file provided with the dataset.

        Parameters
        ----------
        date (list, np.ndarray): list of string dates to be checked, format: year-month-day
        file_path (string): default=None if path is not provided, the default location of the file is assumed

        Returns
        -------
        day_information_df (pd.DataFrame): DataFrame with columns:
            Date (string, date format year-month-day), WorkingDay (boolean), Weekday (string)


        """

        if file_path is None:
            file_path = os.path.abspath(self.dataset_location + "/../" + "day_information.csv")

        day_information_df = None
        if self.weekday_information_df is None:  # if not initialized yet
            self.weekday_information_df = pd.read_csv(file_path)

        if type(date) in [list, np.ndarray]:
            if not all(isinstance(n, str) for n in date):  # if not all dates are strings, convert them
                date = [str(n) for n in date]
            day_information_df = self.weekday_information_df[self.weekday_information_df.Date.isin(date)]
            day_information_df.Date = day_information_df.Date.apply(lambda x: pd.to_datetime(x, format='%Y-%m-%d')).dt.date



        return day_information_df


# copied from the MEED paper "MEED-An-Unsupervised-Multi-Environment-EventDetector-for-Non-Intrusive-Load-Monitoring", same author as CREAM, Daniel Jorde
class Electrical_Metrics():
    """
    Class that contains several functions to compute (approximately) diverse Electrical metrics:

    active_power
    apparent_power
    reative_power
    power_factor
    voltage_current_rms
    single_rms

    """
    def __init__(self):
        pass

    def active_power(self,instant_voltage, instant_current,period_length):
        """
        Active or Real power is the average of instantaneous power.
        P = Sum ( i[n] * v[n] ) / N )
        First we calculate the instantaneous power by multiplying the instantaneous
        voltage measurement by the instantaneous current measurement. We sum the
        instantaneous power measurement over a given number of samples and divide by
        that number of samples.


        Parameters
        ----------
        instant_voltage : ndarray
            Instantaneous Voltage, flat array
        instant_current : ndarray
            Instantaneous Current, flat array
        period_length : int
            Number of samples the features are computed over

        Returns
        -------
        active_power : ndarray
            Active Power array

        """

        instant_current = np.array(instant_current).flatten()
        instant_voltage = np.array(instant_voltage).flatten()

        if len(instant_current) == len(instant_voltage):
            instant_power = instant_voltage * instant_current
            period_length = int(period_length)

            active_power = []
            for i in range(0, len(instant_power), period_length):
                if i + period_length <= len(instant_power):
                    signal_one_period = instant_power[i:int(i + period_length)]
                    active_power_one_period = np.mean(signal_one_period )
                    active_power.append(active_power_one_period)
            active_power = np.array(active_power)
            return active_power

        else:
            raise ValueError("Signals need to have the same length")

    def apparent_power(self, instant_voltage,instant_current,period_length):
        """
        Compute apparent power:
        S = Vrms * Irms

        Parameters
        ----------
        instant_voltage : ndarray
            Instantaneous Voltage, flat array
        instant_current : ndarray
            Instantaneous Current, flat array
        period_length : int
            Number of samples the features are computed over

        Returns
        -------
        apparent_power : ndarray
            Apparent Power array

        """
        if len(instant_current) == len(instant_voltage):

            rms_voltage = self.compute_single_rms(instant_voltage,period_length)
            rms_current = self.compute_single_rms(instant_current,period_length)
            apparent_power = rms_voltage * rms_current
            return apparent_power

        else:
            raise ValueError("Signals need to have the same length")

    def reactive_power(self,apparent_power,active_power):
        """
        Compute reactive power:
        Q = sqrt(S^2 - P^2)

        Parameters
        ----------
        apparent_power : ndarray
            Apparent power, flat array
        active_power : ndarray
            Active power, flat array

        Returns
        -------
        reactive_power : ndarray
            Reactive power, flat array

        """

        if len(apparent_power) == len(active_power):
            reactive_power = np.sqrt((apparent_power * apparent_power) - (active_power * active_power))
            return reactive_power
        else:
            raise ValueError("Signals need to have the same length")


    def compute_power_factor(self,apparent_power,active_power):
        """
        Compute power factor:
        PF = P / S

        Parameters
        ----------
        apparent_power : ndarray
            Apparent power, flat array
        active_power : ndarray
            Active power, flat array

        Returns
        -------
        power_factor : float
            Power factor

        """

        power_factor = active_power / apparent_power
        return power_factor


    def compute_voltage_current_rms(self, voltage, current, period_length):
        """
        Compute Root-Mean-Square (RMS) values for the provided voltage and current.

        Parameters
        ----------
        voltage : ndarray
            Instantaneous Voltage, flat array
        current : ndarray
            Instantaneous Current, flat array
        period_length : int
            Number of samples the features are computed over

        Returns
        -------
        voltage_rms : ndarray
            Voltage RMS values
        current_rms : ndarray
            Current RMS values

        """
        period_length = int(period_length)
        voltage_rms = self.compute_single_rms(voltage, period_length)
        current_rms = self.compute_single_rms(current, period_length)
        return voltage_rms, current_rms


    def compute_single_rms(self,signal,period_length):
        """
        Compute Root-Mean-Square (RMS) values for the provided signal.

        Parameters
        ----------
        signal : ndarray
            Instantaneous Voltage OR Current flat array

        period_length : int
            Number of samples the features are computed over

        Returns
        -------
        signal_rms : ndarray
            RMS values of signal

        """
        rms_values = []
        period_length = int(period_length)
        for i in range(0, len(signal), period_length):
            if i + period_length <= len(signal):
                signal_one_period = signal[i:int(i + period_length)]
                rms_one_period = np.sqrt(np.mean(np.square(signal_one_period))) #rms
                rms_values.append(rms_one_period)
        return np.array(rms_values)
