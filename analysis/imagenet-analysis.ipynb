{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet Pipeline Profiling Analysis\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "None. Exemplary logs are loaded by default. Experiments were run on:\n",
    "\n",
    "* Storage: CEPH HDD\n",
    "* CPU: Intel Xeon E5-2630 v3 8x@2.4GHz\n",
    "* Image: ubuntu-18.04-lts/Openstack\n",
    "* Memory: 80GB DDR4\n",
    "\n",
    "All plots that are not saved with the `save_fig` function were not used in the paper, but may provide a close-up look on specific values.\n",
    "\n",
    "### Meta information\n",
    "\n",
    "* \"first\" preprocessing step of listing the files was removed (check `imagenet_demo.py`) because it did not affect performance in our tests. This should be reintroduced when simulating distributed training, e.g., federated learning, where getting the file locations can actually affect performance\n",
    "\n",
    "* Dataset size: `146.899991342 GB` (`imagenet/ILSVRC/Data/CLS-LOC/train> du -hb .` / 1000 / 1000)\n",
    "* Amount of `.JPEG` files: `1281167` (`imagenet/ILSVRC/Data/CLS-LOC/train> find . -name \"*.JPEG\" | wc -l`)\n",
    "* Avg. filesize: `0.11466107957978935 MB`\n",
    "* Avg. resolution (Imagenet paper): `400x350`\n",
    "* Sample sizes (rounded to second decimal):\n",
    "    * `   0500:        57.33 MB` \n",
    "    * `   1000:       114.66 MB`\n",
    "    * `   2000:       229.32 MB`\n",
    "    * `   4000:       458.64 MB`\n",
    "    * `   8000:       917.29 MB`\n",
    "    * `1281167: 146899.99 MB` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset_samplecount = 1281167\n",
    "running_sample_count = 8000 # full_dataset_samplecount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib import ticker\n",
    "from typing import List\n",
    "# adding previous directory for easier use of library\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from notebookhelper import show_values_on_bars, show_values_on_catplot, save_figure, make_big_number_prettier \\\n",
    "                         , make_big_number_prettier_storage_mb\n",
    "\n",
    "from presto.analysis import StrategyAnalysis \\\n",
    "                       , strat_analysis_from_csv\n",
    "\n",
    "pd.set_option('display.max_rows', 200)\n",
    "plotting_context = \"paper\"\n",
    "default_palette = \"colorblind\"\n",
    "epoch_palette = sns.color_palette(\"YlOrRd\", 3)\n",
    "samples_palette = sns.color_palette(\"icefire\", 15)\n",
    "threads_palette = sns.color_palette(\"tab20\", 4)\n",
    "compression_palette = sns.color_palette(\"Spectral\",3)\n",
    "font_scale = 1.4\n",
    "sns.set(font_scale=font_scale, context=plotting_context)\n",
    "sns.set(rc={\"figure.dpi\":300, 'savefig.dpi':300})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_fig_dir = \"imagenet-pipeline\"\n",
    "\n",
    "def save_fig(name, file_type='pdf'):\n",
    "    save_figure(name, local_fig_dir=local_fig_dir, file_type=file_type)\n",
    "    \n",
    "    \n",
    "log_path = \"/logs/\"\n",
    "path_to_cum_df = f\"{log_path}/full-log_cum-df.csv\"\n",
    "path_to_cum_dstat_df = f\"{log_path}/full-log_cum-dstat-df.csv\"\n",
    "sampling_tag = \"\"\n",
    "\n",
    "analysis = strat_analysis_from_csv(path_to_cum_dstat_df = path_to_cum_dstat_df\n",
    "                                   , path_to_cum_df = path_to_cum_df)\n",
    "cum_dstat_df = analysis.to_cum_dstat_df()\n",
    "cum_df       = analysis.to_cum_df()\n",
    "# need to sort the strategies for some reason as they are not ascending\n",
    "cum_df = cum_df.sort_values(by='split_name')\n",
    "strategies   = list(cum_df.split_name.unique())\n",
    "strategies_renamed = [\"unprocessed\", \"concatenated\", \"decoded\", \"resized\", \"pixel-\\ncentered\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Storage Consumption vs Throughput Tradeoffs\n",
    "### Full dataset, 8 threads, epoch 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_df_temp = cum_df.query(f\"sample_count=={running_sample_count} \\\n",
    "                         and thread_count==8 \\\n",
    "                         and runs_count==0 \\\n",
    "                         and compression_type=='none'\")\n",
    "\n",
    "sample_size_mb_dict = {\n",
    "    \"500\": 58.42\n",
    "  , \"1000\": 116.84\n",
    "  , \"2000\": 233.68\n",
    "  , \"4000\": 467.35\n",
    "  , \"8000\": 934.71\n",
    "  , f\"{full_dataset_samplecount}\": 146899.991342\n",
    "}\n",
    "\n",
    "storage_consumption_comparison = {\n",
    "    \"storage_consumption_mb\": []\n",
    "  , \"sample_count\": []\n",
    "  , \"strategy\": []\n",
    "}\n",
    "\n",
    "def add_to_dict(size, sample_count, label):\n",
    "    '''Short helper'''\n",
    "    storage_consumption_comparison[\"storage_consumption_mb\"] += [size]\n",
    "    storage_consumption_comparison[\"sample_count\"] += [sample_count]\n",
    "    storage_consumption_comparison[\"strategy\"] += [label]\n",
    "    \n",
    "for sample_count in cum_df_temp.sample_count.unique():\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        shard_sizes_mb = cum_df_temp.query(f\"split_name=='{strategy}' and sample_count=={sample_count}\")[\"shard_cum_size_MB\"].to_numpy()\n",
    "        for size_mb in shard_sizes_mb:\n",
    "            if i == 0: # i.e., unprocessed\n",
    "                size_mb = sample_size_mb_dict[str(sample_count)]\n",
    "            add_to_dict(size = size_mb\n",
    "                      , sample_count = sample_count\n",
    "                      , label = strategies[i])\n",
    "\n",
    "storage_df = pd.DataFrame(storage_consumption_comparison)\n",
    "\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "sns.set(palette=default_palette, font_scale=0.9)\n",
    "throughput_color = \"#515151\"\n",
    "marker = 'o'\n",
    "linestyle = \":\"\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(4.5,2))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# throughput plot\n",
    "plot2 = sns.pointplot(x=\"split_name\", y=\"throughput_sps\", data=cum_df_temp,\n",
    "                      ax=ax2, scale=0.75, color=throughput_color, linestyles=linestyle, ci=0.95, marker='o')\n",
    "\n",
    "plot2.set(ylabel=\"Throughput in Samples\\nper Second\")\n",
    "plot2.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=20\n",
    ")\n",
    "ax2.grid(False)\n",
    "legend_elements = [Line2D([0], [0], marker=marker, markersize=5, color=throughput_color, lw=2.5, label='Throughput', linestyle=linestyle)]\n",
    "ax2.legend(handles=legend_elements, loc=\"upper left\", prop={'size': 9})\n",
    "plot2.set(ylim=(0, 2300))\n",
    "\n",
    "plot = sns.barplot(\n",
    "        x=\"strategy\",\n",
    "        y=\"storage_consumption_mb\",\n",
    "        data=storage_df.query(f\"sample_count=={running_sample_count}\"),\n",
    "        ax=ax1\n",
    "    )\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=12\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "show_values_on_bars(plot, h_v=\"v\", space=120000, rotation=5, additional_x_space=[0.05,0,0,0,0]\n",
    "                                                            , additional_space=[50000,0,0,0,0], storage_formatting=True)\n",
    "plot.set(ylabel=\"Storage Consumption\\nin MB\", xlabel=\"\")\n",
    "plot.set(ylim=(0, 1650000))\n",
    "\n",
    "plot.yaxis.set_major_formatter(make_big_number_prettier)\n",
    "plot2.yaxis.set_major_formatter(make_big_number_prettier)\n",
    "save_fig(\"storage-vs-throughput\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Caching Analysis\n",
    "### Full dataset, 8 threads, epoch 0 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(palette=epoch_palette, font_scale=0.8)\n",
    "\n",
    "cum_df_temp = cum_df.query(f\"sample_count=={running_sample_count} \\\n",
    "                         and thread_count==8 and runs_count<=1 and compression_type=='none'\")\n",
    "\n",
    "plt.figure(figsize=(4.5,2))\n",
    "plot = sns.barplot(\n",
    "    x=\"split_name\",\n",
    "    y=\"throughput_sps\",\n",
    "    hue=\"runs_count\",\n",
    "    data=cum_df_temp\n",
    ")\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=10\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "plot.set(xlabel=\"\", ylabel=\"Throughput in Samples\\n per Seconds\")\n",
    "plot.set(ylim=(0, 2300))\n",
    "plot.legend(title=\"Epoch\", ncol=1, labelspacing=0.1, loc=\"upper left\")\n",
    "show_values_on_bars(plot, h_v=\"v\", space=150, rotation=15, additional_space=[0,0,0,100,0\n",
    "                                                                            ,0,0,0,0,0])\n",
    "save_fig(\"caching-over-epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parallelism\n",
    "### Normalizing the processing time per samples and renaming the maximum sample count to \"full\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the per sample processing time for both offline and online processing\n",
    "cum_df[\"per_sample_online_processing_time_s\"] = cum_df[\"online_processing_time_s\"] / cum_df[\"sample_count\"]\n",
    "cum_df[\"per_sample_offline_processing_and_save_time_s\"] = cum_df[\"offline_processing_and_save_time_s\"] / cum_df[\"sample_count\"]\n",
    "# calculate the total processing time for each sample count\n",
    "cum_df[\"total_processing_time_s\"] = cum_df[\"online_processing_time_s\"] + cum_df[\"offline_processing_and_save_time_s\"]\n",
    "# extrapolate based on the full count of images\n",
    "cum_df[\"full_dataset_online_processing_time_s\"] = cum_df[\"per_sample_online_processing_time_s\"] * full_dataset_samplecount\n",
    "cum_df[\"full_dataset_offline_processing_and_save_time_s\"] = cum_df[\"per_sample_offline_processing_and_save_time_s\"] * full_dataset_samplecount\n",
    "cum_df[\"full_dataset_offline_processing_and_save_time_m\"] = cum_df[\"full_dataset_offline_processing_and_save_time_s\"] / 60\n",
    "cum_df[\"full_dataset_offline_processing_and_save_time_h\"] = cum_df[\"full_dataset_offline_processing_and_save_time_m\"] / 60\n",
    "\n",
    "\n",
    "# sum the offline and online time for the full dataset processing time (converted to hours)\n",
    "cum_df[\"full_dataset_processing_time_s\"] = cum_df[\"full_dataset_online_processing_time_s\"] + cum_df[\"full_dataset_offline_processing_and_save_time_s\"]\n",
    "cum_df[\"full_dataset_processing_time_m\"] = cum_df[\"full_dataset_processing_time_s\"] / 60\n",
    "cum_df[\"full_dataset_processing_time_h\"] = cum_df[\"full_dataset_processing_time_m\"] / 60 \n",
    "\n",
    "\n",
    "cum_df_mod = cum_df.copy(deep=True)\n",
    "cum_df_mod.loc[cum_df_mod.sample_count == 1281167, 'sample_count'] = '1.3M (full)'\n",
    "order_dict = { 500: 10, 1000: 11, 2000: 12, 4000: 13, 8000: 14, '1.3M (full)': 15}\n",
    "strategy_order_dict = dict(list(enumerate(strategies)))\n",
    "strategy_order_dict = dict({(y, x) for x, y in strategy_order_dict.items()})\n",
    "order_dict.update(strategy_order_dict)\n",
    "cum_df_mod = cum_df_mod.sort_values(by=['split_name', 'sample_count'], key=lambda x: x.map(order_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_8000_df = cum_df_mod.query(\"sample_count==8000 and compression_type=='none'\")\n",
    "\n",
    "parallel_capability = {\n",
    "      \"thread_count\": []\n",
    "    , \"strategy\": []\n",
    "    , \"speedup\": []\n",
    "    , \"run\": []\n",
    "}\n",
    "\n",
    "def add_capability(thread_count, strategy, speedup, run):\n",
    "    parallel_capability[\"thread_count\"] += [thread_count]\n",
    "    parallel_capability[\"strategy\"] += [strategy]\n",
    "    parallel_capability[\"speedup\"] += [speedup]\n",
    "    parallel_capability[\"run\"] += [run]\n",
    "\n",
    "for strategy in strategies:\n",
    "    for run_count in cum_df.runs_count.unique():\n",
    "        for thread_count in cum_df.thread_count.unique():\n",
    "            throughputs = cum_8000_df.query(f\"thread_count=={thread_count} and split_name=='{strategy}' and runs_count=={run_count}\")[\"throughput_sps\"].to_numpy()\n",
    "            for throughput in throughputs:\n",
    "                avg_step_t1_throughput = cum_8000_df.query(f\"thread_count=={1} and split_name=='{strategy}' and runs_count=={run_count}\")[\"throughput_sps\"].mean()\n",
    "                # not using percentages here as I feel its more intuitive, benchmarking crimes is still followed from my understanding\n",
    "                #if avg_step_t1_throughput < throughput:\n",
    "                #    rel_increase_factor =  throughput / avg_step_t1_throughput - 1\n",
    "                #    add_capability(thread_count, strategy, np.round(rel_increase_factor, 2))\n",
    "                #else:\n",
    "                #    rel_decrease_factor = 1 - throughput / avg_step_t1_throughput\n",
    "                #    add_capability(thread_count, strategy, -np.round(rel_decrease_factor, 2))\n",
    "                ## now using speedup instead of increase/decrease factor\n",
    "                speedup = throughput / avg_step_t1_throughput\n",
    "                add_capability(thread_count, strategy, speedup, run_count)\n",
    "\n",
    "parallel_capability_df = pd.DataFrame(parallel_capability)\n",
    "parallel_capability_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(palette=threads_palette, font_scale=0.8)\n",
    "\n",
    "plt.figure(figsize=(4.5,2))\n",
    "plot = sns.barplot(\n",
    "    x=\"strategy\",\n",
    "    y=\"speedup\",\n",
    "    hue=\"thread_count\",\n",
    "    data=parallel_capability_df.query(\"run==0\")\n",
    ")\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=10\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "#plot.set_title(f\"Caching Speedup\")\n",
    "plot.set(xlabel=\"\", ylabel=\"Speedup\")\n",
    "plot.set(ylim=(0, 8))\n",
    "plot.legend(title=\"Threads\", ncol=1,labelspacing=0.05, loc=(0.815,0.5))\n",
    "save_fig(\"speedup-8000-samples-epoch-0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(palette=threads_palette, font_scale=0.8)\n",
    "\n",
    "plt.figure(figsize=(4.5,2))\n",
    "plot = sns.barplot(\n",
    "    x=\"strategy\",\n",
    "    y=\"speedup\",\n",
    "    hue=\"thread_count\",\n",
    "    data=parallel_capability_df.query(\"run==1\")\n",
    ")\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=10\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "#plot.set_title(f\"Caching Speedup\")\n",
    "plot.set(xlabel=\"\", ylabel=\"Speedup\")\n",
    "plot.set(ylim=(0, 8))\n",
    "plot.legend(title=\"Threads\", ncol=1,labelspacing=0.05, loc=(0.815,0.5))\n",
    "save_fig(\"speedup-8000-samples-epoch-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_df_ord = cum_df.copy(deep=True).query(f\"thread_count==8 and runs_count==0 and split_name!='0-fully-online' and sample_count=={running_sample_count}\")\n",
    "order_dict = { \"none\": 10, \"GZIP\": 11, \"ZLIB\": 12 }\n",
    "cum_df_ord = cum_df_ord.sort_values(by=['compression_type', 'split_name'], key=lambda x: x.map(order_dict))\n",
    "strategies_renamed = [\"concatenated\", \"decoded\", \"resized\", \"pixel-\\ncentered\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "cum_df_temp = cum_df_ord\n",
    "sns.set(palette=compression_palette,font_scale=0.8)\n",
    "marker = 'o'\n",
    "linestyle = [(10,2), (4,5), (1,2)]\n",
    "linestyle2 = [(0,(10,2)), (0,(4,5)), (0,(1,2))]\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(4.5,2))\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "## throughput plot\n",
    "plot2 = sns.lineplot(\n",
    "    x=\"split_name\",\n",
    "    y=\"throughput_sps\",\n",
    "    hue=\"compression_type\",\n",
    "    data=cum_df_temp,\n",
    "    ax=ax2,\n",
    "    size=3,\n",
    "    sizes=(3,3),\n",
    "    marker=marker,\n",
    "    dashes=linestyle,\n",
    "    ci=0.95,\n",
    "    markeredgecolor='black',\n",
    "    style='compression_type')\n",
    "\n",
    "plot2.set(ylabel=\"Throughput in Samples\\nper Second\")\n",
    "ax2.grid(False)\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker=marker, markeredgecolor='#000000', markeredgewidth=0.5, markersize=5, lw=0.9, label='none', color=\"#E49467\", linestyle=linestyle2[0]),\n",
    "    Line2D([0], [0], marker=marker, markeredgecolor='#000000', markeredgewidth=0.5, markersize=5, lw=1.5, label='GZIP', color=\"#F7F7C6\", linestyle=linestyle2[1]),\n",
    "    Line2D([0], [0], marker=marker, markeredgecolor='#000000', markeredgewidth=0.5, markersize=5, lw=2.5, label='ZLIB', color=\"#90C6A6\", linestyle=linestyle2[2])]\n",
    "ax2.legend(handles=legend_elements, loc=(0.005,0.6), prop={'size': 9})\n",
    "plot2.set(ylim=(0, 2300))          \n",
    "\n",
    "plot = sns.barplot(\n",
    "        x=\"split_name\",\n",
    "        y=\"shard_cum_size_MB\",\n",
    "        hue=\"compression_type\",\n",
    "        data=cum_df_temp,\n",
    "        ax=ax1\n",
    "    )\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=10\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "plot.set(ylabel=\"Storage Consumption\\nin MB\", xlabel=\"\")\n",
    "plot.yaxis.set_major_formatter(make_big_number_prettier)\n",
    "show_values_on_bars(plot, h_v=\"v\", space=80000, rotation=15, storage_formatting=True, storage_round=0,\n",
    "                    additional_space=[175000,50000,100000,50000\n",
    "                                     , 85000,75000, 75000,-175000\n",
    "                                     ,     0,-350000,     0,125000]\n",
    "                  , additional_x_space=[ 0.05,0,0,0\n",
    "                                       ,0.025,0,0,0\n",
    "                                       ,    0,0,0,-0.01])\n",
    "\n",
    "plot.set(ylim=(0, 1650000))\n",
    "ax1.get_legend().remove()\n",
    "plot.yaxis.set_major_formatter(make_big_number_prettier)\n",
    "plot2.yaxis.set_major_formatter(make_big_number_prettier)\n",
    "save_fig(\"compressed-storage-vs-throughput\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the per sample processing time for both offline and online processing\n",
    "cum_df_ord[\"per_sample_online_processing_time_s\"] = cum_df_ord[\"online_processing_time_s\"] / cum_df_ord[\"sample_count\"]\n",
    "cum_df_ord[\"per_sample_offline_processing_and_save_time_s\"] = cum_df_ord[\"offline_processing_and_save_time_s\"] / cum_df_ord[\"sample_count\"]\n",
    "# calculate the total processing time for each sample count\n",
    "cum_df_ord[\"total_processing_time_s\"] = cum_df_ord[\"online_processing_time_s\"] + cum_df_ord[\"offline_processing_and_save_time_s\"]\n",
    "# extrapolate based on the full count of images\n",
    "cum_df_ord[\"full_dataset_online_processing_time_s\"] = cum_df_ord[\"per_sample_online_processing_time_s\"] * full_dataset_samplecount\n",
    "cum_df_ord[\"full_dataset_offline_processing_and_save_time_s\"] = cum_df_ord[\"per_sample_offline_processing_and_save_time_s\"] * full_dataset_samplecount\n",
    "cum_df_ord[\"full_dataset_offline_processing_and_save_time_m\"] = cum_df_ord[\"full_dataset_offline_processing_and_save_time_s\"] / 60\n",
    "cum_df_ord[\"full_dataset_offline_processing_and_save_time_h\"] = cum_df_ord[\"full_dataset_offline_processing_and_save_time_m\"] / 60\n",
    "\n",
    "# sum the offline and online time for the full dataset processing time (converted to hours)\n",
    "cum_df_ord[\"full_dataset_processing_time_s\"] = cum_df_ord[\"full_dataset_online_processing_time_s\"] + cum_df_ord[\"full_dataset_offline_processing_and_save_time_s\"]\n",
    "cum_df_ord[\"full_dataset_processing_time_m\"] = cum_df_ord[\"full_dataset_processing_time_s\"] / 60\n",
    "cum_df_ord[\"full_dataset_processing_time_h\"] = cum_df_ord[\"full_dataset_processing_time_m\"] / 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_df_temp = cum_df_ord\n",
    "sns.set(palette=compression_palette, font_scale=0.8)\n",
    "fig, ax1 = plt.subplots(figsize=(4.5,2))\n",
    "plot = sns.barplot(\n",
    "    x=\"split_name\",\n",
    "    y=\"full_dataset_processing_time_h\",\n",
    "    hue=\"compression_type\",\n",
    "    data=cum_df_temp\n",
    ")\n",
    "plot = sns.barplot(\n",
    "    x=\"split_name\",\n",
    "    y=\"full_dataset_offline_processing_and_save_time_h\",\n",
    "    hue=\"compression_type\",\n",
    "    data=cum_df_temp,\n",
    "    color=\"grey\"\n",
    ")\n",
    "\n",
    "plot.set_xticklabels(\n",
    "    strategies_renamed\n",
    "  , rotation=5\n",
    ")\n",
    "plot.tick_params(axis='x', which='major', pad=-1.5)\n",
    "sample_counts = len(cum_df_temp.compression_type.unique())\n",
    "strategies_count = len(cum_df_temp.split_name.unique())\n",
    "\n",
    "for i,bar in enumerate(plot.patches):\n",
    "    if i > (sample_counts * strategies_count):\n",
    "        bar.set_hatch(\"//\")\n",
    "\n",
    "plot.set(xlabel=\"\", ylabel=\"Time in Hours\")\n",
    "plot.set(ylim=(0, 27))\n",
    "\n",
    "handles, labels = plot.get_legend_handles_labels()\n",
    "\n",
    "def add_status_to_handle(handle, index):\n",
    "    compression_count = 3\n",
    "    label = handle.get_label()\n",
    "    if index >= compression_count:\n",
    "        new_label = label + ' (offline)'\n",
    "    else:\n",
    "        new_label = label + ' (online)'\n",
    "    handle.set_label(new_label)\n",
    "    \n",
    "for i, handle in enumerate(handles):\n",
    "    add_status_to_handle(handle,index=i)\n",
    "\n",
    "plot.legend(handles=handles, title=\"\", loc=\"upper left\", ncol=1, labelspacing=0.02, columnspacing=0.3)\n",
    "save_fig(\"compression-processing-time-split\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
